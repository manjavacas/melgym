About MELGYM
************

.. image:: ../_static/images/logo.png
    :align: center
    :scale: 30 %

**MELGYM** is a Python tool designed to facilitate interactive control over `MELCOR 1.8.6 <https://en.wikipedia.org/wiki/MELCOR>`_ simulations.

Every control functionality in MELCOR is determined by *Control Functions* (CFs). However, the batch execution mode of MELCOR makes it difficult to interactively control and modify functions under certain user-defined conditions. 
Control conditions are defined a priori and sometimes requires the concatenation of several CFs that must be done in an unfriendly way.

MELGYM allows the definition of external user-defined controllers, allowing the use of **reinforcement learning** agents or any other custom/external **control** algorithm.

.. figure:: ../_static/images/mdp-simp.png
    :align: center
    :scale: 27 %

\

.. warning:: The MELGEN and MELCOR executables are **NOT INCLUDED** in this package, as they are not freely distributable. The execution of MELGYM requires both executables, which must be located in the respective paths ``melgym/exec/MELGEN`` and ``melgym/exect/MELCOR``.

.. note:: Access to this MEGEN/MELCOR exectuables can be requested via the `Sandia National Laboratories website <https://www.sandia.gov/MELCOR/code-distribution/>`_.

How MELGYM works
================

The control performed by MELGYM is mainly based on MELCOR's **restart** capabilities. The restart process in MELCOR involves:

1. Dumping data about the current state of the simulation into the *restart file* generated by MELGEN (i.e., recording a *simulation checkpoint*).
2. A subsequent warm start from the last recorded simulation state.

.. figure:: ../_static/images/melcor.png
    :align: center
    :scale: 6 %

    The MELGEN/MELCOR execution process

These two steps are performed periodically, according to the restart frequency defined by the user in the MELCOR input configuration.

MELGYM introduces an **intermediate step** between every restart dump + warm start, in which the modification of the control functions or any other user-defined components of the model takes place.

This allows to **discretise** the simulations in small time intervals, adding intermediate steps where the model is continuously modified.

.. figure:: ../_static/images/timeline.png
    :align: center
    :scale: 6 %

    MELGYM involves additional restart steps to enable precise control over MELCOR elements

.. note:: Those elements modifiable by MELGYM are determined by the overwrite capabilities of the MELCOR input. See the `MELCOR code manual <https://www.sandia.gov/MELCOR/publications/>`_ for more information.

Therefore, MELGYM modifies the input model every few simulation cycles. Each time a warm start is performed, the model starts from the last registered state and continues simulating under the new configuration defined by the **external controller**.

.. figure:: ../_static/images/restart.png
    :align: center
    :scale: 20 %

    Control actions modify the current MELCOR model, and are performed just before each warm start

Finally, the following image shows a summary of the implemented functions and the low-level agent-environment interaction.

.. figure:: ../_static/images/functions.png
    :align: center
    :scale: 9 %

    Low-level diagram of the agent-environment interaction loop

Reinforcement learning integration
==================================

Formulation
-----------

Reinforcement learning (RL) algorithms are an especially useful tool for learning control policies through interaction between an agent and a simulated environment.

RL problems consist of the interaction between an *agent* and a dynamic process, called the *environment*, over a discrete sequence of time steps :math:`\mathcal{T} = \{0,1,2,...\}`. Every MELGYM environment corresponds to a simulated MELCOR model with which an agent interacts.

.. figure:: ../_static/images/mdp-melgym.png
    :align: center
    :scale: 9 %

    The Partially Observable Markov Decission Process implemented in MELGYM

At each time step :math:`t`, the agent perceives an *observation* :math:`o_t \in \mathcal{O}`, which includes a subset of the variables that define the current *state* of the simulation :math:`s_t \in \mathcal{S}`. Based on this information, an *action* :math:`a_t \in \mathcal{A}` is performed, modifying the MELCOR model. The updated model is then simulated for a specified period, resulting in a new state :math:`s_{t+1} \in \mathcal{S}` from which a *reward* signal :math:`r_t \in \mathbb{R}` is computed. This reward represents the quality of the transition, thus guiding the agent's learning process.

The mapping between states and actions is determined by a *policy* function :math:`\pi`, such that :math:`a_t \sim \pi(\cdot|s_t)`. The objective of an RL agent is to discover an optimal policy :math:`\pi^*` that maximizes cumulative rewards over time, specifically :math:`G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}`, where :math:`\gamma \in [0,1]` is a discount factor that ponderates future rewards.

When RL is combined with neural networks, it results in *deep* reinforcement learning (DRL) algorithms. These algorithms use a parameterized policy :math:`\pi_\theta`, such as a neural network with weights :math:`\theta`. Through gradient-based optimization, the expected return of the policy :math:`\pi_\theta` is incrementally aligned with that of an optimal policy.

RL control in MELGYM
--------------------

MELGYM enables the integration of DRL-based control in MELCOR simulations. The process followed to implement this control consists of the use of intermediate *restarts* as mentioned above.

.. figure:: ../_static/images/mdp.png
    :align: center
    :scale: 20 %

    Agent-environment interaction loop

Note that the outputs provided by the MELCOR *External Data File* (EDF) are parsed into observations that the agent uses to determine the next control action to be performed, as well as to calculate the associated reward. In addition, actions consist of modifications on the control elements allowed by the MELGYM environment. The observed variables, actions and rewards will depend on the environment.

.. note:: MELGYM adheres to the standard `Gymnasium <https://gymnasium.farama.org/>`_ interface, so its environments implement its typical methdos (e.g. *reset*, *step*, *render*, ...). The definition of the agent depends on the user preferences, but MELGYM is highly integrated with the `Stable-Baselines3 <https://stable-baselines3.readthedocs.io/en/master/>`_ library, so its use is highly encouraged.

.. tip:: After this introduction, go to section :ref:`examples` for a hands-on explanation of how a DRL controller can be used with MELGYM.